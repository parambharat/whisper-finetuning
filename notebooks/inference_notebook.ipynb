{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab24a0e3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 08:40:41.437329: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 08:40:45.372647: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mugan/anaconda3/envs/cscie89/lib::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/bin:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/bin:/home/mugan/anaconda3/envs/whisper_finetuning/lib/\n",
      "2022-12-15 08:40:45.372720: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mugan/anaconda3/envs/cscie89/lib::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/bin:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/bin:/home/mugan/anaconda3/envs/whisper_finetuning/lib/\n",
      "2022-12-15 08:40:45.372725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import gradio as gr\n",
    "import pytube as pt\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcfdb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"parambharat/whisper-tiny-ta\" # change to any one of the available models\n",
    "lang = \"ta\"\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=MODEL_NAME,\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "pipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=lang, task=\"transcribe\")\n",
    "\n",
    "def transcribe(microphone, file_upload):\n",
    "    warn_output = \"\"\n",
    "    if (microphone is not None) and (file_upload is not None):\n",
    "        warn_output = (\n",
    "            \"WARNING: You've uploaded an audio file and used the microphone. \"\n",
    "            \"The recorded file from the microphone will be used and the uploaded audio will be discarded.\\n\"\n",
    "        )\n",
    "\n",
    "    elif (microphone is None) and (file_upload is None):\n",
    "        return \"ERROR: You have to either use the microphone or upload an audio file\"\n",
    "\n",
    "    file = microphone if microphone is not None else file_upload\n",
    "\n",
    "    text = pipe(file)[\"text\"]\n",
    "\n",
    "    return warn_output + text\n",
    "\n",
    "\n",
    "def _return_yt_html_embed(yt_url):\n",
    "    video_id = yt_url.split(\"?v=\")[-1]\n",
    "    HTML_str = (\n",
    "        f'<center> <iframe width=\"500\" height=\"320\" src=\"https://www.youtube.com/embed/{video_id}\"> </iframe>'\n",
    "        \" </center>\"\n",
    "    )\n",
    "    return HTML_str\n",
    "\n",
    "\n",
    "def yt_transcribe(yt_url):\n",
    "    yt = pt.YouTube(yt_url)\n",
    "    html_embed_str = _return_yt_html_embed(yt_url)\n",
    "    stream = yt.streams.filter(only_audio=True)[0]\n",
    "    stream.download(filename=\"audio.mp3\")\n",
    "\n",
    "    text = pipe(\"audio.mp3\")[\"text\"]\n",
    "\n",
    "    return html_embed_str, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "728673dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/gradio/inputs.py:318: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/gradio/deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/gradio/interface.py:332: UserWarning: Currently, only the 'default' theme is supported.\n",
      "  warnings.warn(\"Currently, only the 'default' theme is supported.\")\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/gradio/inputs.py:26: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/gradio/blocks.py:717: UserWarning: api_name predict already exists, using predict_1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Blocks()\n",
    "\n",
    "mf_transcribe = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=[\n",
    "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\", optional=True),\n",
    "        gr.inputs.Audio(source=\"upload\", type=\"filepath\", optional=True),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    layout=\"horizontal\",\n",
    "    theme=\"huggingface\",\n",
    "    title=\"Whisper Tiny Tamil Demo: Transcribe Audio\",\n",
    "    description=(\n",
    "        \"Transcribe long-form microphone or audio inputs with the click of a button! Demo uses the the fine-tuned\"\n",
    "        f\" checkpoint [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}) and ðŸ¤— Transformers to transcribe audio files\"\n",
    "        \" of arbitrary length.\"\n",
    "    ),\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "yt_transcribe = gr.Interface(\n",
    "    fn=yt_transcribe,\n",
    "    inputs=[gr.inputs.Textbox(lines=1, placeholder=\"Paste the URL to a YouTube video here\", label=\"YouTube URL\")],\n",
    "    outputs=[\"html\", \"text\"],\n",
    "    layout=\"horizontal\",\n",
    "    theme=\"huggingface\",\n",
    "    title=\"Whisper Tiny Tamil Demo: Transcribe YouTube\",\n",
    "    description=(\n",
    "        \"Transcribe long-form YouTube videos with the click of a button! Demo uses the the fine-tuned checkpoint:\"\n",
    "        f\" [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}) and ðŸ¤— Transformers to transcribe audio files of\"\n",
    "        \" arbitrary length.\"\n",
    "    ),\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "with demo:\n",
    "    gr.TabbedInterface([mf_transcribe, yt_transcribe], [\"Transcribe Audio\", \"Transcribe YouTube\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6170cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True)\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/mugan/anaconda3/envs/whisper_finetuning/lib/python3.9/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0d4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
