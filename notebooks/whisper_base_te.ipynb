{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JKgmVNgVAD17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !git lfs install\n",
    "# !git clone https://huggingface.co/datasets/parambharat/malayalam_asr_corpus\n",
    "\n",
    "# !add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
    "# !apt update\n",
    "# !apt install -y ffmpeg\n",
    "\n",
    "# !pip uninstall -y transformers datasets \n",
    "# !pip install audiomentations\n",
    "# !pip install git+https://github.com/huggingface/datasets\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip install librosa soundfile\n",
    "# !pip install \"evaluate>=0.3.0\"\n",
    "# !pip install jiwer\n",
    "# !pip install more-itertools\n",
    "# !pip install wandb\n",
    "# !pip install bitsandbytes\n",
    "# !pip install \"bokeh<2.5\"\n",
    "# !pip install \"holoviews[recommended]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZC8tW_tfNo-J",
    "outputId": "639126ea-3e26-4200-e257-1f9ad1be95c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL=True\n",
      "env: WANDB_WATCH=all\n",
      "env: WANDB_NOTEBOOK_NAME=whisper_base_te.ipynb\n"
     ]
    }
   ],
   "source": [
    "%set_env WANDB_LOG_MODEL=True\n",
    "%set_env WANDB_WATCH=all\n",
    "%set_env WANDB_NOTEBOOK_NAME=whisper_base_te.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "R2pNlZvdABCs",
    "outputId": "111bc8db-133e-4a42-adc3-8f38f865ecee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, IterableDatasetDict, load_dataset, interleave_datasets, Audio \n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "import wandb\n",
    "from IPython.display import clear_output\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import TrainerCallback\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from torch.utils.data import IterableDataset\n",
    "from datasets import load_dataset, Audio\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "import tempfile\n",
    "from bokeh.resources import INLINE\n",
    "hv.extension(\"bokeh\", logo=False)\n",
    "\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import jiwer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "clear_output()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "pBO2LqJ4NW3T",
    "outputId": "edde5e59-a398-4e3f-846d-92b2d339c995"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparambharat\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/Documents/whisper-finetuning/notebooks/wandb/run-20221213_103440-2eu5o3v5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/parambharat/whisper_finetuning/runs/2eu5o3v5\" target=\"_blank\">wandering-yogurt-68</a></strong> to <a href=\"https://wandb.ai/parambharat/whisper_finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"whisper_finetuning\", job_type=\"fine-tuning\", group=\"base-te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact = run.use_artifact('parambharat/whisper_finetuning/model-13kya0x9:latest', type='model')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PuTCIEJSBIOM"
   },
   "outputs": [],
   "source": [
    "def load_data_splits(is_streaming=True, stopping_strategy=\"all_exhausted\"):\n",
    "    dataset_dict = {}\n",
    "    \n",
    "    dataset_dict[\"train\"] = load_dataset(\"../data/telugu_asr_corpus/\", split=\"train\", streaming=is_streaming)\n",
    "    dataset_dict[\"test\"] = load_dataset(\"google/fleurs\", \"te_in\", split=\"test\", streaming=True)\n",
    "    dataset_dict[\"test\"] = dataset_dict[\"test\"].rename_column(\"transcription\", \"sentence\")\n",
    "    dataset_dict[\"test\"] = dataset_dict[\"test\"].remove_columns(\n",
    "        [col for col in dataset_dict[\"test\"].features.keys() if col not in [\"audio\", \"sentence\"]])\n",
    "    dataset_dict[\"test\"] = dataset_dict[\"test\"].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p-v-YdYPBMkM"
   },
   "outputs": [],
   "source": [
    "dataset_dict = load_data_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kV_BfJISBPu8"
   },
   "outputs": [],
   "source": [
    "augment_waveform = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=0.3),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.3, leave_length_unchanged=False),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.3)\n",
    "    ,])\n",
    "\n",
    "def augment_dataset(batch):\n",
    "\n",
    "    audio = batch[\"audio\"][\"array\"]\n",
    "    # apply augmentation\n",
    "    augmented_audio = augment_waveform(samples=audio, sample_rate=16000)\n",
    "\n",
    "    batch[\"audio\"][\"array\"] = augmented_audio\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# call augment dataset on the training set\n",
    "dataset_dict[\"train\"] = dataset_dict[\"train\"].map(augment_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "585b0b1c9786490a961fb7fe21998de6",
      "59205c66679d46dcb469f376ac3c2d08",
      "1cc08107ea24411ba51b5f80ca3536a9",
      "aab76a742392448485b9c44c560d2c6a",
      "485aec7e415d4c85817dd69e8e8e8606",
      "96c418ee57bc4af38c1f62f55032467c",
      "5b2d29e3fc8748669994b2c21928acc8",
      "2ad1a4dcfcb448e9a071e4f29f6b2267",
      "b83dec40708c4a9fb909f76c0234e504",
      "27599aeefa014f28914ed6e43c395ae4",
      "f7d051a3864742b283e2bffabd82c297",
      "e32d92759d1a4421a365b64ccbcd8071",
      "8dd598bc6068435e8fcda24f40de3753",
      "521e283ad6f44fe6b31de9d0099c157b",
      "f26579757153406495223827daba0fc3",
      "e65a4af4481340bfbb4f66e66446ccbd",
      "52429f14a0db4372b7f0bb490a4ca13f",
      "d03ed621c29646f8805fbdc9edd18306",
      "7ce3d55d14184399bcf8ea0f99738e2d",
      "0bcf42857cf84f69949aade6f52bca12",
      "ff84fa37cfe746048d65999ac6eddfb7",
      "c4110d5193cd4964b452ab50b07f13f2",
      "4ea37dfe2e5d4b89a43395d4f68093bc",
      "8f1b95d8649b4fefbe95c58a9cf459e9",
      "0ddec1cce79f47d8b32deaef2e69f6a3",
      "36db4a0e11f84b08b25196230b0fc0fe",
      "554260363aa04ee288db28e6468f7a9b",
      "23732cd636d1426abf96b26e3996b13a",
      "a8fcd33287e74aad850ae65af861b488",
      "6e5358ea8a2c40339fb20d368c7591a3",
      "0d8dbb7801a841dc948343e174cc6afd",
      "d4722cbbd67c444a952c7259273ee831",
      "6c45dbfcc6fd4e128eee4430537ff346",
      "a85ac666003e4a2fb78dd862e2f0437d",
      "abe6cd7910ff4e78ad51c9e446334793",
      "6297a4c8aac542ba89212c78a839570b",
      "eaf45260aae44c7bb3464c5f9e88a409",
      "225f02d7fcac4aaa829fff8405c21c6d",
      "00d4bca95d76461b8a5602415f6141bb",
      "83ddf256df7a4a47843406df1a717b81",
      "5589f3777e0744138b0e94e06bc5fc93",
      "fe4344e6d88b43629f3da77c00ee1358",
      "265df923bf8544f5b32b52cc9ba98183",
      "8dd009245ec440448e06e599e4528f1d",
      "81a714946afa49f7a1aa60cf5225c6cf",
      "a9eb479d03114478ba7f8d0d1baa0107",
      "9eb2080030c0476285867e3292ae3a7d",
      "72058d6119224a528713af88898fdb8e",
      "72af4b25685447509279e787792eee3d",
      "4ed1d764a78747aa9f03126e470b1ae4",
      "d6e9a32535f24c358e102ec4a06327d1",
      "093a45a0d74e4d32a76a853dbb7a7d9b",
      "31666af495474df59e276a0afe5645ac",
      "42ffce6c5a174f9ab10bcb2f50526f84",
      "514aba80884745b3ac059fe1d93670a5",
      "e50fbda2b16e4f9ca45f6ef8a7a99d38",
      "cf736a63c73b494687f5e22ea0308545",
      "08e569d63c754c1d8ea8b8034647cdde",
      "310ff5d1575842db9dfbe881e0ba4380",
      "addd53ea56584c7d876603af9f6ac848",
      "b1ce4a12fe564d9d84ac88f7a2d79979",
      "9d237abd294046f694db33dfe3577ae4",
      "7aec746c8d14438698d68b8b54173903",
      "9520a423559d4fc5bce571acebf1c824",
      "12a44d6d605f414ea105864335347e78",
      "5fecb7fbb4a2446fa9e255008b84045a",
      "addf23fce1c84a2bba9a98f5c5dcb6e7",
      "d167644f2d8c4b9eb6a36db675449e33",
      "ace7a92f624748fd90607acc8c8cb1ef",
      "7efe13a1c9c344c4b4c20d5b8f34a36d",
      "41ac27038b434b818e67718739572292",
      "977630eb6e99477b91915f4d7e4520f9",
      "ea3a71cf8c4e45acb22abd571851fe6f",
      "76966903653a489cafe2d43f3a5b888e",
      "3480b0ad43d44e4e8ad79cb7bf38261d",
      "fb2c97b9f1a04668a6b860dedd8470a6",
      "341effee78ed4259a5aee20e782b75fa"
     ]
    },
    "id": "Ed-DU67MBRZM",
    "outputId": "0039bc32-662b-40a8-e75c-32c9fa7a0a76"
   },
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "    \"openai/whisper-base\"\n",
    ")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\n",
    "    \"openai/whisper-base\", \n",
    "     language=\"Telugu\",\n",
    "     task=\"transcribe\",\n",
    "     model_max_length=225\n",
    ")\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-base\",\n",
    "     language=\"Telugu\", \n",
    "     task=\"transcribe\",\n",
    "     model_max_length=225\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3RRzYMlzBS1c"
   },
   "outputs": [],
   "source": [
    "def fix_sentence(sentence):\n",
    "    transcription = sentence\n",
    "  \n",
    "    if transcription.startswith('\"') and transcription.endswith('\"'):\n",
    "        # we can remove trailing quotation marks as they do not affect the transcription\n",
    "        transcription = transcription[1:-1]\n",
    "  \n",
    "    if transcription[-1] not in [\".\", \"?\", \"!\"]:\n",
    "        # append a full-stop to sentences that do not end in punctuation\n",
    "        transcription = transcription + \".\"\n",
    "    transcription = transcription[:-1].translate(str.maketrans('', '', string.punctuation)) + transcription[-1]\n",
    "    return transcription\n",
    "    \n",
    "def prepare_dataset(examples):\n",
    "    # compute log-Mel input features from input audio array \n",
    "    audio = examples[\"audio\"]\n",
    "    \n",
    "    examples[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "    \n",
    "    sentences = fix_sentence(examples[\"sentence\"])\n",
    "    \n",
    "    # encode target text to label ids \n",
    "    examples[\"labels\"] = tokenizer(sentences, max_length=225, truncation=True).input_ids\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5AmKi1H8ciMT"
   },
   "outputs": [],
   "source": [
    "def filter_empty_strings(sentence):\n",
    "    if len(sentence) < 2:\n",
    "        return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w575MV2fciMT"
   },
   "outputs": [],
   "source": [
    "for k in dataset_dict:\n",
    "    dataset_dict[k] = dataset_dict[k].filter(filter_empty_strings, input_columns=[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d_dZciZ4B3dx"
   },
   "outputs": [],
   "source": [
    "for k in dataset_dict:\n",
    "    dataset_dict[k] = dataset_dict[k].map(\n",
    "        prepare_dataset,).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Q563P7EWciMU"
   },
   "outputs": [],
   "source": [
    "dataset_dict[\"train\"] = dataset_dict[\"train\"].shuffle(buffer_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Oy4IjH8TDBdK"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": self.processor.tokenizer.truncate_sequences(feature[\"labels\"])[0]}\n",
    "                          for feature in features]\n",
    "        # pad the labels to max length\n",
    "        \n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\",)\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OEyoMdF4Epw_"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "dbacc7358e0e4fc58929a36918280963",
      "a01ebf345a59434ba755427fb8e7c12e",
      "71cd5d4a24bf44b3912a07b9dba7f8d9",
      "7af830ac1d3942dd89eb66811a3829ea",
      "6ab29d5a279e4fbfb7346a1bc2b56ebf",
      "0fc192efa12741ff84707f9fc3d4e6da",
      "a800f90c3ef44580a4205730f9844a33",
      "9e5d5520013945c8861165cebad82350",
      "1e25852d80f944548aa90c64562b0f34",
      "719d5b78da2342c79c1e9d8857926b19",
      "5c279655de7146ca94613063463a59ed"
     ]
    },
    "id": "QHQYVCO6E589",
    "outputId": "96cd6bc6-d0ee-4a5d-de42-fb2f1bf70e33"
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "#Â evaluate with the 'normalised' WER\n",
    "do_normalize_eval = True\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True, normalize=do_normalize_eval)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True, normalize=do_normalize_eval)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "40aac3a68347490fa3ebc9c9bbd55aae",
      "6db2a6614b4941c49ee54edba7cdba72",
      "d058723ff46d4ca6b22998d090038f4b",
      "90c58238fbc24fec9ccb582172757358",
      "eb8763d486994285a23f20ac416d9878",
      "f219e2a2066e48be90d0f58c3a8d88d3",
      "639178c645d24a8a83d3dd3f3aec93cf",
      "8eee4e19eecd473a84addf98462445c1",
      "f9cd9a318ecd4d4986564f9dc91fe884",
      "e140ef8b151941849d04682a44a3ff7c",
      "0512e64568544b4cbd5a15410b7def05",
      "edd314fa8fe34b94a2a3bd1bcf4ee0ca",
      "b13fc3215dc044d39f4dcbc2d9627775",
      "7c7cdd5d15e844d4ad5268fba2424352",
      "ef8f62a665424f6fb7470152bf20f248",
      "089101dd3b644e149eed24f0ba1607e1",
      "e2fef2e25845469899964ce96ff90d02",
      "b0d5ef3f38b249d78697e0b184eba763",
      "ea41287cacd748c99620f0a05ec3803f",
      "d24fe167b9384b0d843a536f78bdedca",
      "6341d33bae3c4e92a403a135274a4d33",
      "477df107d7494e06b8aadd7fdef76636"
     ]
    },
    "id": "Fzh1uv_tHh6k",
    "outputId": "730a51ac-fdf5-41f3-efd5-27e82e18a6d8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\", use_cache=False)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "e9jrAocYIRbo"
   },
   "outputs": [],
   "source": [
    "# trainer callback to reinitialise and reshuffle the streamable datasets at the beginning of each epoch\n",
    "class ShuffleCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "            pass  # set_epoch() is handled by the Trainer\n",
    "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
    "            train_dataloader.dataset.set_epoch(train_dataloader.dataset._epoch + 1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mWkzBxKFciMV"
   },
   "outputs": [],
   "source": [
    "def load_samples_dataset(dataset, num_samples=100):\n",
    "    samples = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        samples.append(item)\n",
    "        if i == (num_samples-1):\n",
    "            break\n",
    "    sample_dataset = Dataset.from_list(samples)\n",
    "    return sample_dataset\n",
    "\n",
    "def compute_spectrograms(example):\n",
    "    waveform =  example[\"audio\"][\"array\"]\n",
    "    specs = feature_extractor(waveform, sampling_rate=16000, padding=\"do_not_pad\").input_features[0]\n",
    "    return {\"spectrogram\": specs}\n",
    "\n",
    "\n",
    "def record_to_html(sample_record):\n",
    "    audio_array = np.array(sample_record[\"audio\"][\"array\"])\n",
    "    audio_sr = sample_record[\"audio\"][\"sampling_rate\"]\n",
    "    length = sample_record.get(\"length\")\n",
    "    if length:\n",
    "        audio_duration = length\n",
    "    else:\n",
    "        sample_record[\"length\"] = audio_duration = len(audio_array)/audio_sr\n",
    "    audio_duration = sample_record[\"length\"]\n",
    "    audio_spectrogram = np.array(sample_record[\"spectrogram\"])\n",
    "\n",
    "    bounds = (0,0, audio_duration, audio_spectrogram.max())\n",
    "\n",
    "    waveform_int = np.int16(audio_array * 32767)\n",
    "\n",
    "    \n",
    "    \n",
    "    hv_audio = pn.pane.Audio(waveform_int, sample_rate=audio_sr, name='Audio', throttle=500)\n",
    "    \n",
    "    slider = pn.widgets.FloatSlider(end=audio_duration, visible=False, step=0.001)\n",
    "    line_audio = hv.VLine(0).opts(color='black')\n",
    "    line_spec = hv.VLine(0).opts(color='red')\n",
    "    \n",
    "    \n",
    "    slider.jslink(hv_audio, value='time', bidirectional=True)\n",
    "    slider.jslink(line_audio, value='glyph.location')\n",
    "    slider.jslink(line_spec, value='glyph.location')\n",
    "    \n",
    "    time = np.linspace(0, audio_duration, num=len(audio_array))\n",
    "    line_plot_hv = hv.Curve(\n",
    "        (time, audio_array), [\"Time (s)\", \"amplitude\"]).opts(\n",
    "        width=500, height=150, axiswise=True) * line_audio\n",
    "    \n",
    "    hv_spec_gram = hv.Image(\n",
    "        audio_spectrogram, bounds=(bounds), kdims=[\"Time (s)\", \"Frequency (hz)\"]).opts(\n",
    "        width=500, height=150, labelled=[], axiswise=True, color_levels=512)* line_spec\n",
    "    \n",
    "    \n",
    "    combined = pn.Row(hv_audio, hv_spec_gram, line_plot_hv, slider)\n",
    "    audio_html = StringIO()\n",
    "    combined.save(audio_html)\n",
    "    sample_record[\"audio_with_spec\"] = audio_html\n",
    "    return sample_record\n",
    "\n",
    "\n",
    "def dataset_to_records(dataset):\n",
    "    records = []\n",
    "    for item in dataset:\n",
    "        record = {}\n",
    "        sample_record = record_to_html(item)\n",
    "        record[\"audio_with_spec\"] = wandb.Html(sample_record[\"audio_with_spec\"])\n",
    "        record[\"sentence\"] = sample_record[\"sentence\"]\n",
    "        record[\"length\"] =  sample_record[\"length\"]\n",
    "        records.append(record)\n",
    "    records = pd.DataFrame(records)\n",
    "    return records\n",
    "    \n",
    "def decode_predictions(trainer, predictions):\n",
    "    pred_ids = predictions.predictions\n",
    "    pred_str = trainer.tokenizer.batch_decode(pred_ids, skip_special_tokens=True, )\n",
    "    return pred_str\n",
    "\n",
    "\n",
    "def compute_measures(predictions, labels):\n",
    "    measures = [jiwer.compute_measures(ls, ps) for ps, ls in zip(predictions, labels)]\n",
    "    measures_df = pd.DataFrame(measures)[[\"wer\", \"hits\", \"substitutions\", \"deletions\", \"insertions\"]]\n",
    "    return measures_df\n",
    "\n",
    "class WandbProgressResultsCallback(WandbCallback):\n",
    "    def __init__(self, trainer, sample_dataset): \n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "        self.sample_dataset = sample_dataset\n",
    "        self.records_df = dataset_to_records(sample_dataset)\n",
    "        \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        super().on_log(args, state, control, model, logs)\n",
    "        predictions = trainer.predict(self.sample_dataset)\n",
    "        predictions = decode_predictions(self.trainer, predictions)\n",
    "        measures_df = compute_measures(predictions, self.records_df[\"sentence\"].tolist())\n",
    "        records_df = pd.concat([self.records_df, measures_df], axis=1)\n",
    "        records_df[\"prediction\"] = predictions\n",
    "        records_df[\"step\"] = state.global_step\n",
    "        records_table = self._wandb.Table(dataframe=records_df)\n",
    "        self._wandb.log({\"sample_predictions\": records_table})\n",
    "        \n",
    "    def on_save(self, args, state, control, model=None, tokenizer=None, **kwargs):\n",
    "        if self._wandb is None:\n",
    "            return\n",
    "        if self._log_model and self._initialized and state.is_world_process_zero:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                self.trainer.save_model(temp_dir)\n",
    "                metadata = (\n",
    "                    {\n",
    "                        k: v\n",
    "                        for k, v in dict(self._wandb.summary).items()\n",
    "                        if isinstance(v, numbers.Number) and not k.startswith(\"_\")\n",
    "                    }\n",
    "                    if not args.load_best_model_at_end\n",
    "                    else {\n",
    "                        f\"eval/{args.metric_for_best_model}\": state.best_metric,\n",
    "                        \"train/total_floss\": state.total_flos,\n",
    "                    }\n",
    "                )\n",
    "                artifact = self._wandb.Artifact(\n",
    "                    name=f\"model-{self._wandb.run.id}\",\n",
    "                    type=\"model\", metadata=metadata)\n",
    "                for f in Path(temp_dir).glob(\"*\"):\n",
    "                    if f.is_file():\n",
    "                        with artifact.new_file(f.name, mode=\"wb\") as fa:\n",
    "                            fa.write(f.read_bytes())\n",
    "                self._wandb.run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yLQm6aERHsE8"
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../models/whisper-base-te\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=96,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    save_total_limit=4,\n",
    "    warmup_steps=500,\n",
    "    max_steps=5000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "#     fp16_full_eval=True,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=250,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    push_to_hub=True,\n",
    "    remove_unused_columns=False, \n",
    "    ignore_data_skip=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "dc066d934aa544dd93d3c8a5e18d809b",
      "e75b76e0706b42e399451623e37a4cca",
      "0395338f75f748c4b1112050b0e7f5c4",
      "e833a3a2d807478a9ef7c6635da1990f",
      "1c6a784567224eb0bdd7ef160eb39f6b",
      "4b4c468cdd8e4bb0a9b29ed07bf9838b",
      "c46fc08c97f44bdf9db5ccd6f2c53119",
      "506b99f1a9eb4821893b2ef0fa90733f",
      "1870f227d7dc4c93ad5ed7053e3c6f4d",
      "2774f0a240d04400baaa7515eb5d312a",
      "2d8acd4267544ddb8b787abe17c07988"
     ]
    },
    "id": "VffggCK8ciMV",
    "outputId": "756eaa57-895e-4c0c-df04-ca043614c297"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61589c299d654c5da061c5f9826a566a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples_dataset = load_samples_dataset(dataset_dict[\"test\"]).map(compute_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0mzO_xHIX1Q",
    "outputId": "63f51000-43e6-4e4f-ed62-567ed9b9bcad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Documents/whisper-finetuning/notebooks/../models/whisper-base-te is already a clone of https://huggingface.co/parambharat/whisper-base-te. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=samples_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[ShuffleCallback()],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JLbvuqTuciMW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress_callback = WandbProgressResultsCallback(trainer, samples_dataset)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "35ZCc8l-ciMW"
   },
   "outputs": [],
   "source": [
    "trainer.add_callback(progress_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FD6Vq49JPNCw",
    "outputId": "974d5a96-bf64-4f2b-f623-6366f4a48327"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e4576fe5bcd145f387994a1894bfd95b",
      "99ef7cedf5b34a50a5d0c3be0e32d100",
      "1649c2fbe5044522ac076a35c460e8b3",
      "b0ba50de644d4e0ea915e986d0b46eef",
      "e1619227ca04477185b10b594c351d1e",
      "ec22c775e801459484ec5448118d2e41",
      "687f7e37be404dedb42a1c51aad800ba",
      "5ee860dfc4f5431ca0d85bb499c1d1d1",
      "d9290912c32749d6942b91d27a6219ce",
      "8c9311fcfc7943edbf576d87d3ddeb86",
      "537c4272a63545679b0125a29e2e9f40"
     ]
    },
    "id": "-hxbi4vVPpoy",
    "outputId": "5b1bdb7b-120c-49c1-aac6-9887b9f473e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 480000\n",
      "  Num Epochs = 9223372036854775807\n",
      "  Instantaneous batch size per device = 96\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 96\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5000\n",
      "  Number of trainable parameters = 72593920\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 17:43:04, Epoch 2/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.389353</td>\n",
       "      <td>60.710808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.349000</td>\n",
       "      <td>0.308119</td>\n",
       "      <td>52.093476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.279200</td>\n",
       "      <td>0.287376</td>\n",
       "      <td>49.707887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.271973</td>\n",
       "      <td>47.565725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.263174</td>\n",
       "      <td>45.228822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>0.252878</td>\n",
       "      <td>44.303797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.251919</td>\n",
       "      <td>44.595910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.247453</td>\n",
       "      <td>43.719572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.245060</td>\n",
       "      <td>43.330088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.245466</td>\n",
       "      <td>42.648491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-500\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-500/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-500/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-500/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-500/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Saving model checkpoint to /tmp/tmpyb9410v3\n",
      "Configuration saved in /tmp/tmpyb9410v3/config.json\n",
      "Model weights saved in /tmp/tmpyb9410v3/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpyb9410v3/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpyb9410v3/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpyb9410v3/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpyb9410v3/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cabc11f5011474aac03685fbad0ed4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd356e9a23764eac97c3262e430127fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   39494d0..85c5554  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   85c5554..f4945e6  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-1000\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-1000/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-1000/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-1000/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-1000/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Saving model checkpoint to /tmp/tmpor_4np0_\n",
      "Configuration saved in /tmp/tmpor_4np0_/config.json\n",
      "Model weights saved in /tmp/tmpor_4np0_/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpor_4np0_/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpor_4np0_/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpor_4np0_/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpor_4np0_/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70143f66300444b7b049db7a766b70a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b770bc2a85f4ba2a1645414f16e001f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   f4945e6..6bf7505  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   6bf7505..af88da4  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-1500\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-1500/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-1500/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-1500/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-1500/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Saving model checkpoint to /tmp/tmprpph24qb\n",
      "Configuration saved in /tmp/tmprpph24qb/config.json\n",
      "Model weights saved in /tmp/tmprpph24qb/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmprpph24qb/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmprpph24qb/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmprpph24qb/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmprpph24qb/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936d74c17d534c9f8a5dd10700e5f378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244411ede036498585cd957054fe7cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   af88da4..c8727b7  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   c8727b7..b93e1b9  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-2000\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-2000/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-2000/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-2000/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-2000/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Saving model checkpoint to /tmp/tmpbq6s1pzi\n",
      "Configuration saved in /tmp/tmpbq6s1pzi/config.json\n",
      "Model weights saved in /tmp/tmpbq6s1pzi/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpbq6s1pzi/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpbq6s1pzi/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpbq6s1pzi/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpbq6s1pzi/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2e7f8a56b440d9fd5a931dd51d17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f1da162dcd49cdbbe0952c6f98dda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   b93e1b9..4854d13  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   4854d13..4dab6f6  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-2500\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-2500/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-2500/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-2500/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-2500/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-2500/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Deleting older checkpoint [../models/whisper-base-te/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to /tmp/tmpsp_aqo5o\n",
      "Configuration saved in /tmp/tmpsp_aqo5o/config.json\n",
      "Model weights saved in /tmp/tmpsp_aqo5o/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpsp_aqo5o/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpsp_aqo5o/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpsp_aqo5o/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpsp_aqo5o/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79c12c9b71c4d12a6dccc3badb0e0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ca82fd6d4947ceb453d9f83d006cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   4dab6f6..8ff8ebf  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   8ff8ebf..80e01ab  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-3000\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-3000/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-3000/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-3000/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-3000/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Deleting older checkpoint [../models/whisper-base-te/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to /tmp/tmpamtz15oo\n",
      "Configuration saved in /tmp/tmpamtz15oo/config.json\n",
      "Model weights saved in /tmp/tmpamtz15oo/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpamtz15oo/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpamtz15oo/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpamtz15oo/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpamtz15oo/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ed760465ab4f2da7d3d72dbe971fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cbed2386ba432182f9e54a20be0087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   80e01ab..4cfe5a3  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   4cfe5a3..6041797  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-3500\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-3500/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-3500/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-3500/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-3500/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-3500/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Deleting older checkpoint [../models/whisper-base-te/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to /tmp/tmphksou76n\n",
      "Configuration saved in /tmp/tmphksou76n/config.json\n",
      "Model weights saved in /tmp/tmphksou76n/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmphksou76n/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmphksou76n/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmphksou76n/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmphksou76n/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc6b5943c414a56a5612055de7b0635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00804f30405643e99ebc160140504cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   6041797..fda8254  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   fda8254..585c1cf  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-4000\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-4000/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-4000/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-4000/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-4000/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-4000/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Deleting older checkpoint [../models/whisper-base-te/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to /tmp/tmpat02u1cw\n",
      "Configuration saved in /tmp/tmpat02u1cw/config.json\n",
      "Model weights saved in /tmp/tmpat02u1cw/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpat02u1cw/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpat02u1cw/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpat02u1cw/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpat02u1cw/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f484fc0154384f28842b5770258ff912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723ef7cbe9c944919fbc225f30d90973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   585c1cf..b31ab48  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   b31ab48..fac3362  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-4500\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-4500/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-4500/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-4500/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-4500/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-4500/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Deleting older checkpoint [../models/whisper-base-te/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to /tmp/tmpdouwd9lr\n",
      "Configuration saved in /tmp/tmpdouwd9lr/config.json\n",
      "Model weights saved in /tmp/tmpdouwd9lr/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpdouwd9lr/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpdouwd9lr/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpdouwd9lr/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpdouwd9lr/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d60b5ddef3441a2bd4565f8fd6405fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e57a5f2c58944ee9a20bec3bf6ef429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   fac3362..cb2ed8f  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   cb2ed8f..24a634f  main -> main\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/whisper-base-te/checkpoint-5000\n",
      "Configuration saved in ../models/whisper-base-te/checkpoint-5000/config.json\n",
      "Model weights saved in ../models/whisper-base-te/checkpoint-5000/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/checkpoint-5000/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/checkpoint-5000/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/checkpoint-5000/added_tokens.json\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Deleting older checkpoint [../models/whisper-base-te/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to /tmp/tmpbmgu_420\n",
      "Configuration saved in /tmp/tmpbmgu_420/config.json\n",
      "Model weights saved in /tmp/tmpbmgu_420/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpbmgu_420/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpbmgu_420/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpbmgu_420/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpbmgu_420/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ad601cdcc1462e9a355e80c6704986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file last-checkpoint/optimizer.pt:   0%|          | 32.0k/297M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04630b21cb9347d4ae31be4c66463ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/277M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   24a634f..71e01be  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   71e01be..fae9177  main -> main\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../models/whisper-base-te/checkpoint-5000 (score: 42.648490749756576).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n",
      "/root/Documents/whisper-finetuning/notebooks/../models/whisper-base-te is already a clone of https://huggingface.co/parambharat/whisper-base-te. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n",
      "Saving model checkpoint to /tmp/tmpvf51i67k\n",
      "Configuration saved in /tmp/tmpvf51i67k/config.json\n",
      "Model weights saved in /tmp/tmpvf51i67k/pytorch_model.bin\n",
      "Feature extractor saved in /tmp/tmpvf51i67k/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmpvf51i67k/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpvf51i67k/special_tokens_map.json\n",
      "added tokens file saved in /tmp/tmpvf51i67k/added_tokens.json\n",
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   fae9177..ab2d361  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.3170733963012695, metrics={'train_runtime': 63774.1657, 'train_samples_per_second': 7.527, 'train_steps_per_second': 0.078, 'total_flos': 3.11315009568768e+19, 'train_loss': 0.3170733963012695, 'epoch': 2.13})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "sItye0uSabeD"
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"language\": \"te\",\n",
    "    \"model_name\": \"Whisper Base Te - Bharat Ramanathan\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"whisper-event\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sBT42HX_ciMW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../models/whisper-base-te\n",
      "Configuration saved in ../models/whisper-base-te/config.json\n",
      "Model weights saved in ../models/whisper-base-te/pytorch_model.bin\n",
      "Feature extractor saved in ../models/whisper-base-te/preprocessor_config.json\n",
      "tokenizer config file saved in ../models/whisper-base-te/tokenizer_config.json\n",
      "Special tokens file saved in ../models/whisper-base-te/special_tokens_map.json\n",
      "added tokens file saved in ../models/whisper-base-te/added_tokens.json\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}, 'metrics': [{'name': 'Wer', 'type': 'wer', 'value': 42.648490749756576}]}\n",
      "To https://huggingface.co/parambharat/whisper-base-te\n",
      "   ab2d361..a324cfb  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "eb-0bbu6ciMW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435a8c4a999344449d347d9ef42bb978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3259.348 MB of 3259.348 MB uploaded (482.814 MB deduped)\\r'), FloatProgress(value=â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>ââââââââââ</td></tr><tr><td>eval/runtime</td><td>ââââââââââ</td></tr><tr><td>eval/samples_per_second</td><td>ââââââââââ</td></tr><tr><td>eval/steps_per_second</td><td>ââââââââââ</td></tr><tr><td>eval/wer</td><td>ââââââââââ</td></tr><tr><td>train/epoch</td><td>âââââââââââââââââââââââââââââââ</td></tr><tr><td>train/global_step</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/learning_rate</td><td>ââââââââââââââââââââ</td></tr><tr><td>train/loss</td><td>ââââââââââââââââââââ</td></tr><tr><td>train/total_flos</td><td>â</td></tr><tr><td>train/train_loss</td><td>â</td></tr><tr><td>train/train_runtime</td><td>â</td></tr><tr><td>train/train_samples_per_second</td><td>â</td></tr><tr><td>train/train_steps_per_second</td><td>â</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.24547</td></tr><tr><td>eval/runtime</td><td>74.0826</td></tr><tr><td>eval/samples_per_second</td><td>1.35</td></tr><tr><td>eval/steps_per_second</td><td>0.027</td></tr><tr><td>eval/wer</td><td>42.64849</td></tr><tr><td>train/epoch</td><td>2.13</td></tr><tr><td>train/global_step</td><td>5000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1775</td></tr><tr><td>train/total_flos</td><td>3.11315009568768e+19</td></tr><tr><td>train/train_loss</td><td>0.31707</td></tr><tr><td>train/train_runtime</td><td>63774.1657</td></tr><tr><td>train/train_samples_per_second</td><td>7.527</td></tr><tr><td>train/train_steps_per_second</td><td>0.078</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wandering-yogurt-68</strong>: <a href=\"https://wandb.ai/parambharat/whisper_finetuning/runs/2eu5o3v5\" target=\"_blank\">https://wandb.ai/parambharat/whisper_finetuning/runs/2eu5o3v5</a><br/>Synced 6 W&B file(s), 31 media file(s), 139 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221213_103440-2eu5o3v5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weSumH6BciMW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
